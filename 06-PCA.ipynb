{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "#Data Mining [EN.550.436]\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "**Class 6** - Sept 15, 2015\n",
    "\n",
    "- Principal Component Analysis\n",
    "- Lagrange multipliers\n",
    "- Explained variance \n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "- Anonymous voting on Doodle\n",
    "\n",
    "> Use any alias you like at\n",
    "\n",
    ">    http://goo.gl/C8Vz9c\n",
    "\n",
    "- Be honest to help me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- A linear combination of known $\\phi_k(\\cdot)$ functions (basis functions)\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\sum_{k=1}^K \\beta_k\\, \\phi_k(x) $\n",
    "\n",
    "> It's a dot product\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\boldsymbol\\beta^T \\boldsymbol\\phi(x)$ \n",
    "\n",
    ">with $\\boldsymbol{\\beta}=(\\beta_1,\\dots,\\beta_K)^T$\n",
    "\n",
    "> or\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\boldsymbol{}X\\boldsymbol\\beta$\n",
    "\n",
    "> where $X_{ik} = \\phi_k(x_i)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method of Least Squares\n",
    "\n",
    "- At the optimum\n",
    "\n",
    ">$\\displaystyle \\hat\\beta = (X^T X)^{-1} X^T y $\n",
    "\n",
    "- Hat matrix\n",
    "\n",
    ">$\\hat{y} = X\\hat\\beta = H y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combinations\n",
    "\n",
    "- Coefficients mix a given set of basis vectors, functions, images, shapes, ...\n",
    "\n",
    "\n",
    "> Fourier series\n",
    "\n",
    "<img src=files/Periodic_identity_function.gif width=400> \n",
    "<!--<img src=https://upload.wikimedia.org/wikipedia/commons/e/e8/Periodic_identity_function.gif width=400> -->\n",
    "\n",
    "> Discete Cosine Transform (JPEG) \n",
    "\n",
    "<img src=files/DCT_basis_thumb.gif width=200>\n",
    "<!--<img src=http://www.digitude.net/blog/wp-content/uploads/2010/07/DCT_basis_thumb.gif width=200>-->\n",
    "\n",
    "> Spherical Harmonics\n",
    "\n",
    "<img src=files/Spherical_Harmonics.png>\n",
    "<!--<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Spherical_Harmonics.png/300px-Spherical_Harmonics.png>-->\n",
    "\n",
    "\n",
    "- What is a good basis like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Principal Component Analysis</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Learning\n",
    "\n",
    ">|                | Supervised     |         Unsupervised     |\n",
    " |:---------------|:--------------:|:------------------------:|\n",
    " | **Discrete**   | Classification | Clustering               |   \n",
    " | **Continuous** | Regression     | Dimensionality Reduction |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=files/800px-GaussianScatterPCA.png width=300 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions of Maximum Variance\n",
    "\n",
    "- Let $X\\in\\mathbb{R}^N$ be a continuous random variable with 0 mean and covariance matrix $C$. What is the direction of maximum variance?\n",
    "\n",
    "> For any vector $a\\in\\mathbb{R}^N$ \n",
    "\n",
    "> $\\displaystyle \\mathbb{Var}[a^T X] = \\mathbb{E}\\left[(a^T X)(X^T a)\\right] = \\mathbb{E}\\left[a^T(XX^T)\\,a\\right] = a^T C\\,a$\n",
    "\n",
    "> We have to maximize this such that $a^2\\!=\\!1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Optimization\n",
    "\n",
    "- **Lagrange multiplier**: extra term with new parameter $\\lambda$\n",
    "\n",
    "> $\\displaystyle  \\max_{a\\in{}\\mathbb{R}^N} \\left\\{a^T C\\,a - \\lambda\\,(a^2\\!-\\!1)\\right\\}$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial\\lambda} \\rightarrow\\ \\  a^2\\!-\\!1 = 0\\ \\ $  (duh!)\n",
    "\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial a_k} \\rightarrow\\ \\  $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With indices\n",
    "\n",
    "\n",
    "> $\\displaystyle  \\sum_{i,j} a_i C_{ij} a_j - \\lambda\\,\\left(\\sum_i a_i^2 - 1\\right)$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\frac{\\partial a_i}{\\partial a_k} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\frac{\\partial a_j}{\\partial a_k} - 2\\lambda\\,\\left(\\sum_i a_i \\frac{\\partial a_i}{\\partial a_k}\\right) = 0$ \n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\delta_{ik} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\delta_{jk} - 2\\lambda\\,\\left(\\sum_i a_i \\delta_{ik}\\right) = 0$ \n",
    "\n",
    "> $\\displaystyle \\sum_{j} C_{kj} a_j + \\sum_{i} a_i C_{ik}  - 2\\lambda\\,a_k = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And back again...\n",
    "\n",
    "- With vectors and matrices\n",
    "\n",
    "> $\\displaystyle  Ca + C^Ta - 2\\lambda a = 0$\n",
    "\n",
    "> but $C$ is symmetric \n",
    "\n",
    "> $\\displaystyle  Ca = \\lambda a $\n",
    "\n",
    "- Eigenproblem !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- The value of maximum variance is\n",
    "\n",
    "> $\\displaystyle  a^TCa = a^T \\lambda a = \\lambda a^Ta = \\lambda$\n",
    "\n",
    "> the largest eigenvalue $\\lambda_1$\n",
    "\n",
    "- The direction of maximum variance is the corresponding eigenvector $a_1$\n",
    "\n",
    "> $\\displaystyle  Ca_1 = \\lambda_1 a_1 $\n",
    "\n",
    "- This is the **1st Principal Component** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Principal Component\n",
    "\n",
    "- Direction of largest variance uncorrelated to 1st PC\n",
    "\n",
    "> $\\displaystyle  \\max_{a\\in{}\\mathbb{R}^N} \\left\\{a^T C\\,a - \\lambda\\,(a^2\\!-\\!1) - \\lambda'(a^T C\\,a_1) \\right\\}$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle 2Ca - 2\\lambda{}a-\\lambda'Ca_1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Multiply by $a_1^T\\cdot$\n",
    "\n",
    "> $\\displaystyle 2a_1^TCa - 2a_1^T\\lambda{}a-a_1^T\\lambda'Ca_1 = 0$\n",
    "\n",
    "> $\\displaystyle 0 - 0 - \\lambda'\\lambda_1 = 0 \\ \\ \\rightarrow\\ \\  \\lambda'=0$\n",
    "\n",
    "- Eigenproblem \n",
    "\n",
    "> $\\displaystyle  Ca = \\lambda a $\n",
    "\n",
    "- Solution $\\lambda_2$ and $a_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA \n",
    "\n",
    "- In general\n",
    "\n",
    "> Let $\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_N$ be the eigenvalues of $C$ and $\\hat{e}_1,\\dots,\\hat{e}_N$ the corresponding eigenvectors\n",
    "\n",
    "> $\\displaystyle  C = \\sum_{k=1}^N\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) $\n",
    "\n",
    "> With diagonal $\\Lambda$ matrix of the eigenvalues and an $E$ matrix of $[\\hat{e}_1, \\dots, \\hat{e}_N]$\n",
    "\n",
    "> $\\displaystyle  C = E\\ \\Lambda\\ E^T$\n",
    "\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> If keeping only $K<N$ eigenvectors, the best approximation is taking the first $K$ PCs\n",
    "\n",
    "> $\\displaystyle  C \\approx \\sum_{k=1}^K\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) =  E_K\\Lambda_KE_K^T$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Adding Variances\n",
    "\n",
    "- If $X$ and $Y$ random variables are independent and $Z=X\\pm{}Y$, then \n",
    "\n",
    "> $\\mathbb{Var}[Z]=\\mathbb{Var}[X]+\\mathbb{Var}[Y]$\n",
    "\n",
    "\n",
    "- Proof\n",
    "\n",
    "> $\\mathbb{Var}[Z]=\\mathbb{E}\\big[(Z-\\mu_Z)^2\\big] $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[Z^2\\big] - 2\\mathbb{E}[Z]\\mu_Z + \\mu_{z}^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[Z^2\\big] - \\mu_{z}^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[(X\\pm{}Y)^2\\big] - \\left(\\mu_{X\\pm{}Y}\\right)^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[(X\\pm{}Y)^2\\big] - \\left(\\mu_{X}\\pm{}\\mu_Y\\right)^2 $\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[X^2\\pm{}2XY+Y^2\\big] - \\left(\\mu_{X}^2\\pm{}2\\mu_X\\mu_Y+\\mu_Y^2\\right)$\n",
    "\n",
    "> $\\ \\ = \\mathbb{E}\\big[X^2\\big] \\pm 2\\,\\mathbb{E}\\big[XY\\big] + \\mathbb{E}\\big[Y^2\\big] - \\left(\\mu_{X}^2\\pm{}2\\mu_X\\mu_Y+\\mu_Y^2\\right)$\n",
    "\n",
    "> $\\ \\ =\\mathbb{E}\\big[X^2\\big]-\\mu_X^2 +\\mathbb{E}\\big[Y^2\\big]-\\mu_Y^2 \\pm 2\\left(\\mathbb{E}\\big[XY\\big] -\\mu_X\\mu_Y\\right) $\n",
    "\n",
    "\n",
    "> $\\ \\ =\\mathbb{Var}[X]+\\mathbb{Var}[Y]\\pm 0 $\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Coordiante System\n",
    "\n",
    "- The $E$ matrix of eigenvectors is a rotation, $E\\,E^T = I$\n",
    "\n",
    "> $\\displaystyle  Z = E^T\\, X $\n",
    "\n",
    "\n",
    "- A truncated set of eigenvectors $E_K$ defines a projection\n",
    "\n",
    "> $\\displaystyle  Z_K = E_K^T\\, X $\n",
    "\n",
    "> and\n",
    "\n",
    "> $\\displaystyle  X_K = E_K E_K^T\\, X = P_K\\,X $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Projections\n",
    "\n",
    "- If the square of a matrix is equal to itself\n",
    "\n",
    "> $\\displaystyle  P^2 = P $\n",
    "\n",
    "- For example, projecting on the $\\hat{e}$ unit vector\n",
    "\n",
    "<img src=files/Y7Gx8.png align=right width=250>\n",
    "\n",
    "> Scalar times vector\n",
    "\n",
    "> $\\displaystyle  r' = \\hat{e}\\left(\\hat{e}^T r\\right) = \\hat{e} \\beta_r$\n",
    "\n",
    "> Or  projection of vector $r$\n",
    "\n",
    "> $\\displaystyle  r' = \\left(\\hat{e}\\,\\hat{e}^T\\right)r = P\\,r$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> $\\displaystyle  C \\approx C_K = \\sum_{k=1}^K\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) = \\sum_{k=1}^K\\ \\lambda_k\\,P_k$\n",
    "\n",
    "- And the remaining eigenvectors span the subspace with the least variance\n",
    "\n",
    "> $\\displaystyle  C - C_K = %\\sum_{l=K+1}^N\\ \\lambda_l\\left(\\hat{e}_l\\,\\hat{e}_l^T\\right) =\n",
    "\\sum_{l=K+1}^N\\ \\lambda_l\\,P_l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples\n",
    "\n",
    "- Set of $N$-vectors with expectation value (or average, cf. Bessel correction) 0 arranged in $X=\\left[x_1, x_2, \\dots\\right]$ \n",
    "\n",
    "> Sample covariance matrix is\n",
    "\n",
    ">$\\displaystyle C \\propto X X^T = \\sum_i x_i x_i^T$\n",
    "\n",
    "- Singular Value Decomposition (SVD)\n",
    "\n",
    ">$\\displaystyle X = U W V^T$\n",
    "\n",
    "> where $U^TU=I$, $W$ is diagonal, and $V^TV=I$\n",
    "\n",
    "- Hence\n",
    "\n",
    ">$\\displaystyle C \\propto UWV^T\\ VWU^T = U W^2 U^T$\n",
    "\n",
    "> So if $C=E\\Lambda E^T$, then $E = U$ and $ \\Lambda \\propto W^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample from Bivariate Normal \n",
    "\n",
    "- See previous lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "[[ 0.60804685]\n",
      " [ 3.11883901]]\n",
      "Covariance\n",
      "[[ 6.25414391  6.77913345]\n",
      " [ 6.77913345  8.87159359]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAE4CAYAAAAuFPo7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIASQHkEIID2CEEB6/w+4hhT0B9V6TAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9ec20b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "# generate many 2D (column) vectors\n",
    "S = norm.rvs(0,1,(2,20))\n",
    "S[0,:] *= 4  # scale axis 0\n",
    "f = +pi/4    # rotate by 45 degrees\n",
    "R = array([[cos(f), -sin(f)],\n",
    "           [sin(f),  cos(f)]]) \n",
    "X = R.dot(S)\n",
    "X += np.array([[1],[3]]) # shift\n",
    "\n",
    "figure(figsize=(5,5)); xlim(-15,15); ylim(-15,15);\n",
    "plot(X[0,:],X[1,:],'o',alpha=0.9)\n",
    "\n",
    "# subtract sample mean\n",
    "avg = mean(X, axis=1).reshape(X[:,1].size,1)\n",
    "X -= avg\n",
    "# sample covariance matrix\n",
    "C = X.dot(X.T) / (X[0,:].size-1)\n",
    "print \"Average\\n\", avg\n",
    "print \"Covariance\\n\", C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.77121723, -0.63657206],\n",
       "        [ 0.63657206, -0.77121723]]), array([  0.65856495,  14.46717255]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, E = np.linalg.eig(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.63657206, -0.77121723],\n",
       "        [-0.77121723,  0.63657206]]), array([ 14.46717255,   0.65856495]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E, L, E_same = np.linalg.svd(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.dot(E.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose( E.T, np.linalg.inv(E) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.63657206, -0.77121723],\n",
       "        [-0.77121723,  0.63657206]]), array([ 14.46717255,   0.65856495]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, W, V = np.linalg.svd(X)\n",
    "U, W**2 / (X[0,:].size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ np.allclose( U.dot(U.T), np.eye(U[:,0].size) ), \n",
    "  np.allclose( V.dot(V.T), np.eye(V[:,0].size) )  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.63657206,  0.77121723],\n",
       "        [-0.77121723, -0.63657206]]), array([ 13.74381393,   0.6256367 ]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=X[:,0].size)\n",
    "pca.fit(X.T) # different convention !!!\n",
    "# pca.transform(X.T)\n",
    "pca.components_.T, pca.explained_variance_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
