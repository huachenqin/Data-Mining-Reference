{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining [EN.550.436]\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "**Class 12** - Oct 6, 2015\n",
    "\n",
    "- Classification problems\n",
    "- Naive Bayes Classifier\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "\n",
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "\n",
    ">$T = \\big\\{ (x_i, C_i) \\big\\}$ where $x_i\\in \\mathbb{R}^d$ and $C_i$ is the known class membership \n",
    "\n",
    "> **Query set**\n",
    "\n",
    ">$Q = \\big\\{ x_i \\big\\}$ where $x_i\\in \\mathbb{R}^d$ \n",
    "\n",
    "> For example,\n",
    "> blood tests ($x$) and sick/healty ($C$) - we want to predict if a new patient is sick based on the available measurements\n",
    "\n",
    "- Similar to regression but with discrete categories to classify into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Methods\n",
    "\n",
    "- $k$-NN\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- Logistic regression\n",
    "- Decisions trees\n",
    "- Random forests\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Iris Dataset\n",
    "\n",
    "We'll use this data set available in [scikit-learn](http://scikit-learn.org/stable/index.html), see [this](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) page for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples\n",
    "\n",
    "More [exercises](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#examples-using-sklearn-neighbors-kneighborsclassifier) such as classification of [digits](http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html) are available at http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "- Using Bayes' rule to infer discrete classes $C_k$ for a given $\\boldsymbol{x}$ set of features\n",
    "\n",
    ">$\\displaystyle P(C_k|\\boldsymbol{x}) = \\frac{1}{Z}\\ \\pi(C_k)\\,{\\cal{}L}_{\\boldsymbol{x}}(C_k)$ \n",
    "\n",
    "\n",
    "- Naively assuming the features are independent \n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) = \\prod_i^d p(x_i|C_k)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Learning\n",
    "\n",
    "- Say for Gaussian likelihoods, we simply estimate the sample mean and variance of all features for each class $k$\n",
    "\n",
    ">$\\displaystyle p(x_i|C_k) = G(x_i;\\mu_k, \\sigma^2_k)$\n",
    "\n",
    "- We have to also pick some prior for the classes\n",
    "\n",
    "> Using uniform or based on frequency of points in training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Estimation\n",
    "\n",
    "- Look for maximum of the posterior\n",
    "\n",
    "\n",
    ">$\\displaystyle \\hat{k} =  \\mathrm{arg}\\max_k \\left[ \\pi_k \\prod_i^d G(x_i;\\mu_k, \\sigma^2_k)\\right]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyData(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = MyData()\n",
    "o.data = array([1,2,3])\n",
    "o.target = array([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 classes: [0 1 2]\n",
      "0 [ 5.006  3.418  1.464  0.244] [ 2.0294      2.37126667  0.49173333  0.18773333]\n",
      "1 [ 5.936  2.77   4.26   1.326] [ 4.35173333  1.60833333  3.60666667  0.63873333]\n",
      "2 [ 6.588  2.974  5.552  2.026] [ 6.60426667  1.69873333  4.97493333  1.23206667]\n"
     ]
    }
   ],
   "source": [
    "# unique known classes in training set\n",
    "classes = np.unique(iris.target)\n",
    "print 'There are %d classes:' % len(classes), classes\n",
    "\n",
    "# calc feature means and variances for each class\n",
    "param = dict() # we save them in this dictionary\n",
    "for k in classes:\n",
    "    members = (iris.target == k) # boolean array\n",
    "    num = members.sum()    # True:1, False:0\n",
    "    prior = num / float(iris.target.size)\n",
    "    X = iris.data[members,:] # slice out members\n",
    "    mu = X.mean(axis=0)      # calc mean\n",
    "    X -= mu\n",
    "    var = (X*X).sum(axis=0) / (X[0,:].size-1)\n",
    "    param[k] = (num, prior, mu, var) # save results\n",
    "    if True: print k, mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 8\n"
     ]
    }
   ],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in classes:\n",
    "        num, prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu\n",
    "        d2 = diff*diff / var / 2 \n",
    "        p = prior * np.exp(-d2.sum())\n",
    "        if p > pmax:\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    k_pred[i] = kmax\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.data.shape[0],(iris.target != k_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150 points : 6\n"
     ]
    }
   ],
   "source": [
    "# run sklearn's version - read up on differences if interested\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.data.shape[0],(iris.target != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "- Features are automatically treated correctly relative to each other\n",
    "\n",
    "> For example, measuring similar things in different units? 1m vs 1mm\n",
    "\n",
    "> The estimated mean and variance puts them on a meaningful scale\n",
    "\n",
    "- Independence is a strong assumption and for no good reason\n",
    "\n",
    "> Actually it helps with the computational costs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Naive Bayes\n",
    "\n",
    "- Use the provided [training](files/Class12-Train.csv) and [query](files/Class12-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n",
    "- Place post-it on your laptop to indicate your status and if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes\n",
    "\n",
    "> Taking the negative logarithm of the likelihoods we compare\n",
    "\n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma_1^{-1}(x\\!-\\!\\mu_1) + \\ln|\\Sigma_1|$ vs.\n",
    "\n",
    ">$\\displaystyle (x\\!-\\!\\mu_2)^T\\,\\Sigma_2^{-1}(x\\!-\\!\\mu_2) + \\ln|\\Sigma_2|$\n",
    "\n",
    "> If the difference is lower than a threshold, we classify it accordingly\n",
    "\n",
    "- This is called **Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called **Linear Discriminant Analysis**\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "\n",
    "> Good, if we don't have enough data, for example...\n",
    "\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA and LDA\n",
    "\n",
    "- Use the provided [training](files/Class12-Train.csv) and [query](files/Class12-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n",
    "- Place post-it on your laptop to indicate your status and if you need help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done already?\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster by getting rid of the 'for' loops, etc.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
